---
title: "Machine Learning Exercise"
author: "Gek Paard"
date: "Friday, March 13, 2015"
output: html_document
---

## Introduction
In the MOOC course "Practical Machine Learning" of the Johns Hopkins University the project is to investigate the Weight Lifting Exercises Dataset which is generously provided by the investigators. See their paper: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) Stuttgart, Germany: ACM SIGCHI, 2013. See also http://groupware.les.inf.puc-rio.br/har.  
The dataset consists of measurements by sensors on six individuals who performed the Unilateral Dumbbell Biceps Curl. The measurements were made by sensors on the forearm, biceps, belt and dumbbell. The curl had to be performed in five different ways. This manner is found in the variable "classe" in the training set. Class "A" is the good one, and the classes "B", "C", "D" and "E" are wrong performances. The subjects were superviced by trained personell who made sure that the exercises where performed in the correct right or wrong manner.
The purpose of this exercise is to predict from the measurements the manner in which they did the exercises.

##Investigation
First of all I loaded the required packages needed for this investigation. 
```{r, results='hide'}
library(caret)
library(randomForest)
```
Then I imported the training set, which was already downloaded in the workspace. I made a training set and a validation set. I also imported the testset, with which eventually the final prediction 
```{r}
trainset  <- read.csv("pml-training.csv")
inTrain   <- createDataPartition(trainset$classe, p = 3/4)[[1]]
traindata <- trainset[ inTrain,]
valdata   <- trainset[-inTrain,]
testset   <- read.csv ("pml-testing.csv")
dim(trainset)
```
As can be seen there a lot of variables. Many of them wil not be relevant predictors. The first 7 variables can not be relevant, because they are about who performed the exercise, on which time, the following number etc. So I removed them from the training set. Then I set the abundance of NA's to zero's, because randomForest can't cope with NA's. (I choose randomForest because the train function of caret takes a very long time.) Then I removed all the variables with little or no variation from the dataset. They can't not be important predictors. 
```{r}
traindata   <- trainset
traindata   <- traindata[,-(1:7)]
traindata[is.na(traindata)] <- 0
nzv         <- nearZeroVar(traindata)
traindata   <- traindata[-nzv]
```
Then I checked for correlations in the remaining variables. There are several.  
```{r}
M <- abs(cor(traindata[,-53]))
diag(M) <- 0
correlations <- which(M> 0.8, arr.ind=T)
correlations

```
If a variable correlates with only one other variable I choose to remove the one with the least variation. See the plots below. If variables correlate with more than one other variables I choose to keep the one which has the most correlations. For instance the variable roll-belt correlates with yaw-belt, total-accel-belt, accel-belt-y and accel-belt-z. But yaw-belt only correlates to roll-belt. roll-belt, total-accel-belt, accel-belt-y and accel-belt-ze correlates to each other.     
```{r}
smallTrain <- traindata[,c(18,19)]
prComp <- prcomp(smallTrain)
plot(prComp$x[,1], prComp$x[,2], xlab= "gyros_arm_x (18)", ylab = "gyros_arm_x (19)")
smallTrain <- traindata[,c(21,24)]
prComp <- prcomp(smallTrain)
plot(prComp$x[,1], prComp$x[,2], xlab= "accel_arm_x (21)", ylab = "magnet_arm_x (24)")
smallTrain <- traindata[,c(25,26)]
prComp <- prcomp(smallTrain)
plot(prComp$x[,1], prComp$x[,2], xlab= "magnet_arm_y (25)", ylab = "magnet_arm_Z (26)")
smallTrain <- traindata[,c(28,34)]
prComp <- prcomp(smallTrain)
plot(prComp$x[,1], prComp$x[,2], xlab= "pitch_dumbbell (28)", ylab = "accel_dumbbell_x (34)")
smallTrain <- traindata[,c(29,36)]
prComp <- prcomp(smallTrain)
plot(prComp$x[,1], prComp$x[,2], xlab= "yaw_dumbbell (29)", ylab = "accel_dumbbell_z (36)")
```
```{r}
traindata <- traindata[,-45]
traindata <- traindata[,-36]
traindata <- traindata[,-34]
traindata <- traindata[,-33]
traindata <- traindata[,-31]
traindata <- traindata[,-26]
traindata <- traindata[,-24]
traindata <- traindata[,-19]
traindata <- traindata[,-11]
traindata <- traindata[,-10]
traindata <- traindata[,-9]
traindata <- traindata[,-8]
traindata <- traindata[,-4]
traindata <- traindata[,-3]
dim(traindata)
```
There are now 38 potential predictors available, which is fortunate because the function randomForest can only handle 53 predictors. I use random forest, because it has a high accuray. I only made a models with randomForest because random forest with train takes hours to make a model.    
I checked with the validation set (and confusionMatrix) if the model is good.  
```{r}
set.seed(3436)
modFit1rF <- randomForest(classe ~ ., data=traindata)
predictionClassedata  <- predict(modFit1rF, valdata)
confusionMatrix(predictionClassedata, valdata$classe)
varImp(modFit1rF)
```
The accuracy is very good. Not shown here, but I got the same prediction and accuracy with the train function with method "rf". The function varImp shows the importance of the predictors. I want to check if als the variables are needed for a good prediction. This time I will use only the variables with an importance of 500 and higher from the train function:
```{r}
modFit2rF <- randomForest(classe ~ roll_belt + magnet_dumbbell_z +  pitch_forearm + pitch_belt          + 
                                   magnet_dumbbell_y + roll_forearm + magnet_dumbbell_x + magnet_belt_z +
                                   magnet_belt_y + accel_dumbbell_y, 
                                   data=traindata)
predictionClassedata  <- predict(modFit2rF, valdata)
confusionMatrix(predictionClassedata, valdata$classe)
varImp(modFit2rF)
```
This still looks like a perfect fit. Now I remove the lowest important predictor, magnet-belt-y, from the predictors.  
```{r}
modFit3rF <- randomForest(classe ~ roll_belt + magnet_dumbbell_z + pitch_forearm + pitch_belt +
                                   magnet_dumbbell_y + roll_forearm + magnet_dumbbell_x       + 
                                   magnet_belt_z + accel_dumbbell_y,
                                   data=traindata)
predictionClassedata  <- predict(modFit3rF, valdata)
confusionMatrix(predictionClassedata, valdata$classe)
varImp(modFit3rF)
```
The function randomForest still gives a perfect model. I removed now the magnet-belt-z from the predictors.
```{r}
modFit4rF <- randomForest(classe ~ roll_belt + magnet_dumbbell_z + pitch_forearm + pitch_belt +
                                   magnet_dumbbell_y + roll_forearm + magnet_dumbbell_x       + 
                                   accel_dumbbell_y,
                                   data=traindata)
predictionClassedata  <- predict(modFit4rF, valdata)
confusionMatrix(predictionClassedata, valdata$classe)
varImp(modFit4rF)

```
magnet-dumbbell-x is now the least important predictor. I removed it from the list of predictors.
```{r}
modFit5rF <- randomForest(classe ~ roll_belt + magnet_dumbbell_z + pitch_forearm + pitch_belt +
                                   magnet_dumbbell_y + roll_forearm + accel_dumbbell_y,
                                   data=traindata)
predictionClassedata  <- predict(modFit5rF, valdata)
confusionMatrix(predictionClassedata, valdata$classe)
varImp(modFit5rF)
```
I predicted with the first and the last model the outcome on the testset. 
```{r}
#check on testset
predict(modFit1rF, testset)
predict(modFit5rF, testset)
```
They appear to be the same.  


#Conclusion
I selected seven variables out of 159 possible ones, and had eventually an accuracy of 99.8%. The expected out-of-sample error should be about 0.2%. The result gave me 20 points.  
.    
.   
